{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "import random\n",
    "import codecs\n",
    "import re\n",
    "import glob\n",
    "import string\n",
    "import io\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC,NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "File handeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comments = open(\"F:\\\\DOCUMENTS\\\\Python\\\\myfiles\\\\NLTK\\\\positive.txt\",encoding='utf-8').read()\n",
    "neg_comments = open(\"F:\\\\DOCUMENTS\\\\Python\\\\myfiles\\\\NLTK\\\\negative.txt\",encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Line append into arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comments_list = list(pos_comments.split('\\n'))\n",
    "neg_comments_list = list(neg_comments.split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Feature Extraction for Uni-Gram and Bi-Gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_features_unigram(words):\n",
    "    unigram_dict = dict((word,True) for word in words.split('\\u0020'))     \n",
    "    return unigram_dict\n",
    "\n",
    "\n",
    "def create_word_features_bi_gram(words):\n",
    "    raw = []\n",
    "    bi_gram = []\n",
    "    i=0\n",
    "    \n",
    "    for w in words.split('\\u0020'):\n",
    "        raw.append(w)\n",
    "\n",
    "    while i<(len(raw)-1):\n",
    "        bi_gram.append(raw[i] + \" \"+raw[i+1])\n",
    "        i+=1\n",
    "     \n",
    "    bigram_dict = dict((word,True) for word in bi_gram)\n",
    "    return bigram_dict\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Positive and Negative features array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word_features_uni = []\n",
    "neg_word_features_uni = []    \n",
    "pos_word_features_bi = []\n",
    "neg_word_features_bi = []    \n",
    "\n",
    "\n",
    "for i in pos_comments_list:\n",
    "    pos_word_features_uni.append((create_word_features_unigram(i),\"positive\"))\n",
    "    pos_word_features_bi.append((create_word_features_bi_gram(i),\"positive\"))\n",
    "    \n",
    "for i in neg_comments_list:\n",
    "    neg_word_features_uni.append((create_word_features_unigram(i),\"negative\"))\n",
    "    neg_word_features_bi.append((create_word_features_bi_gram(i),\"negative\"))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Setup trainning and testing test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_features_uni = pos_word_features_uni + neg_word_features_uni\n",
    "random.shuffle(all_word_features_uni)\n",
    "trainning_set_uni = all_word_features_uni[:200]\n",
    "test_set_uni = all_word_features_uni[200:290]\n",
    "\n",
    "all_word_features_bi = pos_word_features_bi + neg_word_features_bi\n",
    "random.shuffle(all_word_features_bi)\n",
    "trainning_set_bi = all_word_features_bi[:200]\n",
    "test_set_bi = all_word_features_bi[200:290]\n",
    "\n",
    "# print(\" \",len(all_word_features_uni))\n",
    "# print(\" \",len(all_word_features_bi))\n",
    "# print(\" \",len(pos_word_features_uni),len(neg_word_features_uni))\n",
    "# print(\" \",len(pos_word_features_bi) ,len(neg_word_features_bi))\n",
    "# print(pos_word_features_bi[155])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Naive Bayes Accuracy check for Uni-Gram, Bi-Gram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_train_uni = NaiveBayesClassifier.train(trainning_set_uni)\n",
    "accuracy_test_uni = nltk.classify.util.accuracy(classifier_train_uni,test_set_uni)\n",
    "print(\"NaiveBayes classifier UniGram: \", (accuracy_test_uni * 100))\n",
    "classifier_train_uni.show_most_informative_features(5)\n",
    "\n",
    "classifier_train_bi = NaiveBayesClassifier.train(trainning_set_bi)\n",
    "accuracy_test_bi = nltk.classify.util.accuracy(classifier_train_bi,test_set_bi)\n",
    "print(\"\\nNaiveBayes classifier Bi-Gram : \", (accuracy_test_bi * 100))\n",
    "classifier_train_bi.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Logisitc Regression Classifier Accuracy check for Uni-Gram, Bi-Gram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression_classifier_uni = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier_uni.train(trainning_set_uni)\n",
    "print(\"Logistic Regression classifier Uni Gram: \",(nltk.classify.accuracy(LogisticRegression_classifier_uni,test_set_uni))*100)\n",
    "\n",
    "\n",
    "LogisticRegression_classifier_bi = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier_bi.train(trainning_set_bi)\n",
    "print(\"Logistic Regression classifier Bi-Gram: \",(nltk.classify.accuracy(LogisticRegression_classifier_bi,test_set_bi))*100)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "SVC classifier Accuracy check for Uni-Gram, Bi-Gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearSVC_classifier_uni = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier_uni.train(trainning_set_uni)\n",
    "print(\"SVM classifier Uni Gram: \",(nltk.classify.accuracy(LinearSVC_classifier_uni,test_set_uni))*100)\n",
    "\n",
    "LinearSVC_classifier_bi = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier_bi.train(trainning_set_bi)\n",
    "print(\"SVM classifier Algo Bi-Gram: \",(nltk.classify.accuracy(LinearSVC_classifier_bi,test_set_bi))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Testing for Positive,Negative comments in Unigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_comment_1 = '''අපි ඔබ සමඟ සිටිමු'''\n",
    "test_com_1 = []\n",
    "for r in Test_comment_1.split('\\u0020'):\n",
    "    test_com_1.append(r)\n",
    "test_dict_1 = dict([(word,True) for word in test_com_1 ])\n",
    "print(Test_comment_1 +\" (Unigram): \"+ classifier_train_uni.classify(test_dict_1))\n",
    "\n",
    "\n",
    "test_comment_2 = '''අපිට දැන් අප්පිරියයි'''\n",
    "testing_2 = []\n",
    "for r in test_comment_2.split('\\u0020'):\n",
    "    testing_2.append(r)\n",
    "test_dict_2 = dict([(word,True) for word in testing_2 ])\n",
    "print(test_comment_2 +\" (Unigram): \"+ classifier_train_uni.classify(test_dict_2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Testing for Positive,Negative comments in Bi-Gram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_comment_1 = '''අපි ඔබ සමඟ සිටිමු'''\n",
    "count = 0\n",
    "test_com_1 = []\n",
    "for r in Test_comment_1.split('\\u0020'):\n",
    "    test_com_1.append(r)\n",
    "bi_grammed_1 = []\n",
    "\n",
    "while count<(len(test_com_1)-1):\n",
    "        bi_grammed_1.append(test_com_1[count] + \" \"+test_com_1[count+1])\n",
    "        count+=1\n",
    "test_dict_1 = dict([(word,True) for word in bi_grammed_1 ])\n",
    "print (Test_comment_1 +\" (Bi-Gram) : \"+classifier_train_bi.classify(test_dict_1))\n",
    "\n",
    "\n",
    "test_comment_2 = '''අපිට දැන් අප්පිරියයි'''\n",
    "count = 0\n",
    "test_com_2 = []\n",
    "for r in test_comment_2.split('\\u0020'):\n",
    "    test_com_2.append(r)    \n",
    "bi_grammed_2 = []\n",
    "while count<(len(test_com_2)-1):\n",
    "        bi_grammed_2.append(test_com_2[count] + \" \"+test_com_2[count+1])\n",
    "        count+=1\n",
    "test_dict_2 = dict([(word,True) for word in bi_grammed_2 ])\n",
    "print (test_comment_2 +\" (Bi-Gram) : \"+classifier_train_bi.classify(test_dict_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VoteClassifier(ClassifierI):\n",
    "#     def __init__(self,*classifiers):\n",
    "#         self._classifiers = classifiers\n",
    "\n",
    "#     def classify(self, features):\n",
    "#         votes = []\n",
    "#         for c in self._classifiers:\n",
    "#             v = c.classify(features)\n",
    "#             votes.append(v)\n",
    "#         return mode(votes)\n",
    "\n",
    "#     def confidence(self,features):\n",
    "#         votes = []\n",
    "#         for c in self._classifiers:\n",
    "#             v = c.classify(features)\n",
    "#             votes.append(v)\n",
    "\n",
    "#         choice_votes = votes.count(mode(votes))\n",
    "#         conf = choice_votes / len(votes)\n",
    "#         return conf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voted_classifier = VoteClassifier(classifier_train_uni,\n",
    "#                                   LogisticRegression_classifier,\n",
    "#                                   classifier_train_bi\n",
    "                                  \n",
    "#                                  )\n",
    "# print(\"voted_classifier Naive Bayes Algo: \",(nltk.classify.accuracy(voted_classifier,trainning_set_uni))*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bihara_dict = {\n",
    "    'Title': 'Shrungaaraya   Mahinda My3 Ranil ;-)',\n",
    "    'View_Count': '2738691', \n",
    "    'Likes': '5350',\n",
    "    'DisLikes': '1733', \n",
    "    'VideoID': 'uJRG1512ygc', \n",
    "    'Comment': ['මොනා උනත් උ ජාතක කරපු අයගෙ වැරදි තමයි ඉතින් රට බෙර ගත්තු නායකට මෙ අපහස කරන්නේ තොපි වගේ අඩු කුලේ පර හැත්ත තවත් සිටිද ඇ හිකි හිකි හිකි ', 'බල්ලන්ගේ වැඩ කේලේහේ ගුනු දන්නේ පර බල්ලේ', 'අපි ඔබ සමඟ සිටිමු', 'ඒකනන් පට්ට බං', 'Mawa හදනව unga ඇඟිලි kadanna ඕනි  මිනිසුන්ට mada ගහන්න හොඳ නැහැ  ma විදිහට  ..', 'Samanaluntanam hari sathutu ethi😂😂', 'මාව subscribe කරන හැම\\u200cෙදනාම මම subscribe කරනවා\\n100% trusted , subscribe කරල comment 1 අනිවා\\nදාගෙන යන්න ,දැකපු ගමන් මම subscribe  කරනවා\\n(මම subscribe නොකරුවොත් ඔයාලට පුලුවන්\\nunsubscribe  කරන්න ) අපි එකතු \\u200cෙවලා subscribe 5000ක්\\nහදා ගමු bro ලා...thanks for all !!!', 'පට්ටයි කොල්ලෝ ❤❤', 'අම්මෝ😂', 'කෝමේකෙ අර බොරැවට වුඩබුරන කනිතුල්බල්ල ,අඩුව මොකද ජෙප්පන්ගෙ වැඩනේද..', 'අපේ සුපිරි ක්\\u200dරිකට් තරු සහ ඔවුන්ගේ සුරූපී බිරියන්...          https://youtu.be/wFtO9oXQIKg', 'උපහාසයට නොව අපහාසයට ලක්කිරිමක් යකෝ😀😀😀', 'හොදට තේරුම් අරන් තියනවා අපේ අප්පච්චිට  ඕන නැටුමකට ඔට්ටුයි කියලායි සමනලයෙකුත් නෙවෙඉ  තාමත් වැඩ පුලුවන් කියලා තේරුම්  ගත්තට ගොඩාආආආඅක් ස්තුති', 'pattama lassanai aaa welldone bro', 'These dogs soo good', 'Elam bro', 'Piumi & kaushi th add una nam patta machooo 👍', 'ගතියක් තියෙනවා .හොදයි .', 'මේවා හදපු උන්ට හෙන ගහන්න ඕන..', 'lankave\"gon\"video', 'ලස්සනට කරලා තියෙනවා', 'aiyo mkda me', 'Mahinda mahathage rupe nethi unanam hodai anith ewntanam a de gelapenawamai 100/100kma', 'මෙක දාපු..කාලකණ්ණි යා...කවුද', 'Kowalski\\n\\n\\n\\n\\n\\n\\nkalakanni  para ballo .dapan u be ammage deewal.', 'Sri.Lankanmanikandan2020💺🤴🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁🦁❇❇❇❇✴✴✴✴✴✴✴✴✴✴✴✴✡✡🕉🤴💺', 'all are sri lanka`s clowns', 'ඔක්කොම අවුලක් ne ban eth මහින්ද මහත්තයා ගත්ත එක set wen ne ban රට bereagaththa miniha mona වැරැදි තිබුණත්', 'aka patta bro patteta😘😘😘😘', 'aiyooo ane ai me baninne? fun ekatane danne.... gd job guy... funny one', '😂😂😂😂😂😂  අයියෝ   ලස්සනයි', 'අඩුකුලේ පරහැත්ත තමයි මේ විදියට අපහස කරන්නේ රෝඩියෝ .අහිකුණ්ඨිකයන්', 'Supiri 😂😂😂', 'Ammooo eka', 'Niyamai... ape rata gena katha krl vadak ne..👏👏👏', '😂😂😂 දෙන්නටම මඩ ගහලා', 'එල් එල් ඕයි.ගැම්ම', '😂😂😂😂😂😂', 'mara lassanai', 'Dancers all one poudumay😂😈😨😤', 'MAWAH THAMAI LANKAWEH RAJEA DANCE 2018 ALA ALA', 'Mula amataka krapu musala hatta', 'kauri monawa kiuwath me rara mahinda rajura bara wewa', 'ඔබට ජයෙන් ජය බුදු සරණයි. https://goo.gl/Zfj1hZ', 'කවුරු උනත් මිනිසුන් ඒ අය කරපු දේවල් ඒ අයට තියෙයි අනික මේ අය අතර රටට සේවයක් කරපු අයත් ඉන්නවා රට ආපයකින් බේරගත්ත අයත් ඉන්නවා වැරදි දාහක් අස්සෙ එක හොදක් හරි දකින්න පුරුදු වෙන්න මිනිසුන්ට අපහාස කරලා මඩ ගහලා අකුසල් පුරවගන්න එපා ඒක මහ පවක්', 'Meka num naraka vadak', 'Thendi patty puri mon', 'Karapu eka ponnayek', 'Shok ane oyala...', 'me wage dewal dana un hire daanna uni..', 'පට්ට ආතල් එකක් 0777192697', 'www.bruise lee', 'kalakanni para ballo dapan ube ammage deewal ..pakya.', 'Top', 'If \\n X', 'daru senr', 'balako ape janapati mhinda koma haduwath handi...😍😍😍', '😂😂😂😂', 'පවිකාරයින්ගේ  පවි  වැඩ', 'Hahahahaha', 'චාටර් බන්', 'නරක. වැඩක්', 'Athal bn meka.....', '😢😢😢😢😢', 'supri ea', 'Ammoo aka', 'meka hadapu video edit soffweya eke nama mokakda bn', 'නියමයි.හලෝ.එචිචර.චාටර්.නැ.', 'Super 😥😥😥😥', 'mun honda naha tamar aath oya widiyata waradi kiyana eka tamage pahathkama pradarsanaya kireemak. ....', '😂😂😂😂', \"Mahenda ayi Anna dalda malegwa Ra's padennda modya\", 'hitagena\"\"ati\"loku\"         vadak\"\"kare\"kiya\"metana\"inna\"eka\"amati\"\"kenek\"karapu\"deyak\"oyalata\"karanna\"puluvanda\"vikara\"ova\"\"\"\"\"\"\"da\"gnnawa\"\"ara\"sunil\"pererata', 'Kohomath mahinda weddek thamai', 'ponna kariyo ammata hukanna kiyahan hadapu ekage ammata polimak danna ennam', 'Ane melody eka godak lassanai.me melody eka thiyena song ekak thiyenawada', 'Ammo eka', '\\u2063තද  තද', 'ලබකත්', 'එල එල', 'very nice song, කොහොමද මේක https://youtu.be/aU7I0v0muns', 'danne akak thibbama nadde yakoo', 'Rani kuwat niyamai wauuuuuuuuuuu😙😙😙😙😙😙😙😙😙😆😗😆😆😆😆🌹🌹🌹🌹🌹🌹🌹🌹🌹🌹👍👍👍👍👍👍👍👍👍👍👍👍💪💪💪💪💪💪', 'Hahaha', 'Siresanaapai  pradana dancer', 'mara hina brooo', 'tho pilawa yuddayan bara gattha charithawalata mahemadasalakanne', 'He hee 😂😂😂😂', '👌👌👌👌👌👌👌', 'horahatekre-ratakapuyakku', 'Nidahase jewath wenna rata berala  deviyek wage minisekuta mewage apahasa karana unge amma appath mewagem apahasayata lakwenawata kemathi athi buru weda karanna epa manussayek wela', 'Vimal viravanshaya mn ekata gala penava', 'Apith hoyanne sungaraya awul karapu ekawai', 'JVP   thamay meyalage katu kala abarila enne', 'mula amathaka wela wage thopita neeee', 'සුපිරි.........', 'Ane mandanne ne', 'ana manda😑😑😑😑😑😑', 'Kawda meka haduwa ponaya.minissunta mada gahanna apa..ponnaya..'], \n",
    "    '_id': '5d9ad961112acd12bffdf83d'\n",
    "}\n",
    "\n",
    "comments = bihara_dict.get('Comment')\n",
    "print(len(comments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "84\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "pos_count = 0\n",
    "neg_count = 0\n",
    "for i in comments:\n",
    "    Test_comment_1 = i\n",
    "    test_com_1 = []\n",
    "    for r in Test_comment_1.split('\\u0020'):\n",
    "        test_com_1.append(r)\n",
    "    test_dict_1 = dict([(word,True) for word in test_com_1 ])\n",
    "    last_classifier = classifer_detail(classifier_train_uni.classify(test_dict_1),\n",
    "                                   classifier_train_bi.classify(test_dict_1),\n",
    "                                   LogisticRegression_classifier_uni.classify(test_dict_1),\n",
    "                                   LogisticRegression_classifier_bi.classify(test_dict_1),\n",
    "                                   LinearSVC_classifier_uni.classify(test_dict_1),\n",
    "                                   LinearSVC_classifier_bi.classify(test_dict_1)\n",
    "                                  )\n",
    "#     print(last_classifier)\n",
    "    if (last_classifier) == 'negative':\n",
    "        neg_count+=1\n",
    "    elif (last_classifier) == 'positive':\n",
    "        pos_count+=1\n",
    "        \n",
    "print (pos_count) \n",
    "print (neg_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test_comment_1 = comments[2]\n",
    "# print (Test_comment_1)\n",
    "# test_com_1 = []\n",
    "# for r in Test_comment_1.split('\\u0020'):\n",
    "#     test_com_1.append(r)\n",
    "# test_dict_1 = dict([(word,True) for word in test_com_1 ])\n",
    "# last_classifier = classifer_detail(classifier_train_uni.classify(test_dict_1),\n",
    "#                                    classifier_train_bi.classify(test_dict_1),\n",
    "#                                    LogisticRegression_classifier_uni.classify(test_dict_1),\n",
    "#                                    LogisticRegression_classifier_bi.classify(test_dict_1),\n",
    "#                                    LinearSVC_classifier_uni.classify(test_dict_1),\n",
    "#                                    LinearSVC_classifier_bi.classify(test_dict_1)\n",
    "#                                   )\n",
    "\n",
    "# print(last_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classifer_detail(NBclassifer_Uni,NBclassifer_Bi,LR_Uni,LR_Bi,SVM_Uni,SVM_Bi):\n",
    "    \n",
    "    positive_votes = 0\n",
    "    negative_votes = 0\n",
    "    \n",
    "    \n",
    "    if NBclassifer_Uni == 'positive':\n",
    "        positive_votes +=1\n",
    "    else:\n",
    "        negative_votes +=1\n",
    "        \n",
    "    if NBclassifer_Bi == 'positive':\n",
    "        positive_votes +=1\n",
    "    else:\n",
    "        negative_votes +=1\n",
    "\n",
    "        \n",
    "    if LR_Uni == 'positive':\n",
    "        positive_votes +=1\n",
    "    else:\n",
    "        negative_votes +=1\n",
    "        \n",
    "    if LR_Bi == 'positive':\n",
    "        positive_votes +=1\n",
    "    else:\n",
    "        negative_votes +=1    \n",
    "        \n",
    "        \n",
    "    if SVM_Uni == 'positive':\n",
    "        positive_votes +=1\n",
    "    else:\n",
    "        negative_votes +=1\n",
    "        \n",
    "    if SVM_Bi == 'positive':\n",
    "        positive_votes +=1\n",
    "    else:\n",
    "        negative_votes +=1    \n",
    "        \n",
    "    if positive_votes>negative_votes:\n",
    "        return 'positive'\n",
    "    elif positive_votes<negative_votes:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'positive'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
