{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "import random\n",
    "import codecs\n",
    "import re\n",
    "import glob\n",
    "import string\n",
    "import io\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC,NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "File handeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comments = open(\"F:\\\\DOCUMENTS\\\\Python\\\\myfiles\\\\NLTK\\\\positive.txt\",encoding='utf-8').read()\n",
    "neg_comments = open(\"F:\\\\DOCUMENTS\\\\Python\\\\myfiles\\\\NLTK\\\\negative.txt\",encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Line append into arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comments_list = list(pos_comments.split('\\n'))\n",
    "neg_comments_list = list(neg_comments.split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Feature Extraction for Uni-Gram and Bi-Gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_features_unigram(words):\n",
    "    unigram_dict = dict((word,True) for word in words.split('\\u0020'))     \n",
    "    return unigram_dict\n",
    "\n",
    "\n",
    "def create_word_features_bi_gram(words):\n",
    "    raw = []\n",
    "    bi_gram = []\n",
    "    i=0\n",
    "    \n",
    "    for w in words.split('\\u0020'):\n",
    "        raw.append(w)\n",
    "\n",
    "    while i<(len(raw)-1):\n",
    "        bi_gram.append(raw[i] + \" \"+raw[i+1])\n",
    "        i+=1\n",
    "     \n",
    "    bigram_dict = dict((word,True) for word in bi_gram)\n",
    "    return bigram_dict\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Positive and Negative features array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word_features_uni = []\n",
    "neg_word_features_uni = []    \n",
    "pos_word_features_bi = []\n",
    "neg_word_features_bi = []    \n",
    "\n",
    "\n",
    "for i in pos_comments_list:\n",
    "    pos_word_features_uni.append((create_word_features_unigram(i),\"positive\"))\n",
    "    pos_word_features_bi.append((create_word_features_bi_gram(i),\"positive\"))\n",
    "    \n",
    "for i in neg_comments_list:\n",
    "    neg_word_features_uni.append((create_word_features_unigram(i),\"negative\"))\n",
    "    neg_word_features_bi.append((create_word_features_bi_gram(i),\"negative\"))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Setup trainning and testing test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_features_uni = pos_word_features_uni + neg_word_features_uni\n",
    "random.shuffle(all_word_features_uni)\n",
    "trainning_set_uni = all_word_features_uni[:200]\n",
    "test_set_uni = all_word_features_uni[200:290]\n",
    "\n",
    "all_word_features_bi = pos_word_features_bi + neg_word_features_bi\n",
    "random.shuffle(all_word_features_bi)\n",
    "trainning_set_bi = all_word_features_bi[:200]\n",
    "test_set_bi = all_word_features_bi[200:290]\n",
    "\n",
    "# print(\" \",len(all_word_features_uni))\n",
    "# print(\" \",len(all_word_features_bi))\n",
    "# print(\" \",len(pos_word_features_uni),len(neg_word_features_uni))\n",
    "# print(\" \",len(pos_word_features_bi) ,len(neg_word_features_bi))\n",
    "# print(pos_word_features_bi[155])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Naive Bayes Accuracy check for Uni-Gram, Bi-Gram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_train_uni = NaiveBayesClassifier.train(trainning_set_uni)\n",
    "accuracy_test_uni = nltk.classify.util.accuracy(classifier_train_uni,test_set_uni)\n",
    "print(\"NaiveBayes classifier UniGram: \", (accuracy_test_uni * 100))\n",
    "classifier_train_uni.show_most_informative_features(5)\n",
    "\n",
    "classifier_train_bi = NaiveBayesClassifier.train(trainning_set_bi)\n",
    "accuracy_test_bi = nltk.classify.util.accuracy(classifier_train_bi,test_set_bi)\n",
    "print(\"\\nNaiveBayes classifier Bi-Gram : \", (accuracy_test_bi * 100))\n",
    "classifier_train_bi.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Logisitc Regression Classifier Accuracy check for Uni-Gram, Bi-Gram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression_classifier_uni = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier_uni.train(trainning_set_uni)\n",
    "print(\"Logistic Regression classifier Uni Gram: \",(nltk.classify.accuracy(LogisticRegression_classifier_uni,test_set_uni))*100)\n",
    "\n",
    "\n",
    "LogisticRegression_classifier_bi = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier_bi.train(trainning_set_bi)\n",
    "print(\"Logistic Regression classifier Bi-Gram: \",(nltk.classify.accuracy(LogisticRegression_classifier_bi,test_set_bi))*100)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "SVC classifier Accuracy check for Uni-Gram, Bi-Gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearSVC_classifier_uni = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier_uni.train(trainning_set_uni)\n",
    "print(\"SVM classifier Uni Gram: \",(nltk.classify.accuracy(LinearSVC_classifier_uni,test_set_uni))*100)\n",
    "\n",
    "LinearSVC_classifier_bi = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier_bi.train(trainning_set_bi)\n",
    "print(\"SVM classifier Algo Bi-Gram: \",(nltk.classify.accuracy(LinearSVC_classifier_bi,test_set_bi))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Testing for Positive,Negative comments in Unigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_comment_1 = '''à¶…à¶´à·’ à¶”à¶¶ à·ƒà¶¸à¶Ÿ à·ƒà·’à¶§à·’à¶¸à·”'''\n",
    "test_com_1 = []\n",
    "for r in Test_comment_1.split('\\u0020'):\n",
    "    test_com_1.append(r)\n",
    "test_dict_1 = dict([(word,True) for word in test_com_1 ])\n",
    "print(Test_comment_1 +\" (Unigram): \"+ classifier_train_uni.classify(test_dict_1))\n",
    "\n",
    "\n",
    "test_comment_2 = '''à¶…à¶´à·’à¶§ à¶¯à·à¶±à·Š à¶…à¶´à·Šà¶´à·’à¶»à·’à¶ºà¶ºà·’'''\n",
    "testing_2 = []\n",
    "for r in test_comment_2.split('\\u0020'):\n",
    "    testing_2.append(r)\n",
    "test_dict_2 = dict([(word,True) for word in testing_2 ])\n",
    "print(test_comment_2 +\" (Unigram): \"+ classifier_train_uni.classify(test_dict_2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Testing for Positive,Negative comments in Bi-Gram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_comment_1 = '''à¶…à¶´à·’ à¶”à¶¶ à·ƒà¶¸à¶Ÿ à·ƒà·’à¶§à·’à¶¸à·”'''\n",
    "count = 0\n",
    "test_com_1 = []\n",
    "for r in Test_comment_1.split('\\u0020'):\n",
    "    test_com_1.append(r)\n",
    "bi_grammed_1 = []\n",
    "\n",
    "while count<(len(test_com_1)-1):\n",
    "        bi_grammed_1.append(test_com_1[count] + \" \"+test_com_1[count+1])\n",
    "        count+=1\n",
    "test_dict_1 = dict([(word,True) for word in bi_grammed_1 ])\n",
    "print (Test_comment_1 +\" (Bi-Gram) : \"+classifier_train_bi.classify(test_dict_1))\n",
    "\n",
    "\n",
    "test_comment_2 = '''à¶…à¶´à·’à¶§ à¶¯à·à¶±à·Š à¶…à¶´à·Šà¶´à·’à¶»à·’à¶ºà¶ºà·’'''\n",
    "count = 0\n",
    "test_com_2 = []\n",
    "for r in test_comment_2.split('\\u0020'):\n",
    "    test_com_2.append(r)    \n",
    "bi_grammed_2 = []\n",
    "while count<(len(test_com_2)-1):\n",
    "        bi_grammed_2.append(test_com_2[count] + \" \"+test_com_2[count+1])\n",
    "        count+=1\n",
    "test_dict_2 = dict([(word,True) for word in bi_grammed_2 ])\n",
    "print (test_comment_2 +\" (Bi-Gram) : \"+classifier_train_bi.classify(test_dict_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VoteClassifier(ClassifierI):\n",
    "#     def __init__(self,*classifiers):\n",
    "#         self._classifiers = classifiers\n",
    "\n",
    "#     def classify(self, features):\n",
    "#         votes = []\n",
    "#         for c in self._classifiers:\n",
    "#             v = c.classify(features)\n",
    "#             votes.append(v)\n",
    "#         return mode(votes)\n",
    "\n",
    "#     def confidence(self,features):\n",
    "#         votes = []\n",
    "#         for c in self._classifiers:\n",
    "#             v = c.classify(features)\n",
    "#             votes.append(v)\n",
    "\n",
    "#         choice_votes = votes.count(mode(votes))\n",
    "#         conf = choice_votes / len(votes)\n",
    "#         return conf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voted_classifier = VoteClassifier(classifier_train_uni,\n",
    "#                                   LogisticRegression_classifier,\n",
    "#                                   classifier_train_bi\n",
    "                                  \n",
    "#                                  )\n",
    "# print(\"voted_classifier Naive Bayes Algo: \",(nltk.classify.accuracy(voted_classifier,trainning_set_uni))*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bihara_dict = {\n",
    "    'Title': 'Shrungaaraya   Mahinda My3 Ranil ;-)',\n",
    "    'View_Count': '2738691', \n",
    "    'Likes': '5350',\n",
    "    'DisLikes': '1733', \n",
    "    'VideoID': 'uJRG1512ygc', \n",
    "    'Comment': ['à¶¸à·œà¶±à· à¶‹à¶±à¶­à·Š à¶‹ à¶¢à·à¶­à¶š à¶šà¶»à¶´à·” à¶…à¶ºà¶œà·™ à·€à·à¶»à¶¯à·’ à¶­à¶¸à¶ºà·’ à¶‰à¶­à·’à¶±à·Š à¶»à¶§ à¶¶à·™à¶» à¶œà¶­à·Šà¶­à·” à¶±à·à¶ºà¶šà¶§ à¶¸à·™ à¶…à¶´à·„à·ƒ à¶šà¶»à¶±à·Šà¶±à·š à¶­à·œà¶´à·’ à·€à¶œà·š à¶…à¶©à·” à¶šà·”à¶½à·š à¶´à¶» à·„à·à¶­à·Šà¶­ à¶­à·€à¶­à·Š à·ƒà·’à¶§à·’à¶¯ à¶‡ à·„à·’à¶šà·’ à·„à·’à¶šà·’ à·„à·’à¶šà·’ ', 'à¶¶à¶½à·Šà¶½à¶±à·Šà¶œà·š à·€à·à¶© à¶šà·šà¶½à·šà·„à·š à¶œà·”à¶±à·” à¶¯à¶±à·Šà¶±à·š à¶´à¶» à¶¶à¶½à·Šà¶½à·š', 'à¶…à¶´à·’ à¶”à¶¶ à·ƒà¶¸à¶Ÿ à·ƒà·’à¶§à·’à¶¸à·”', 'à¶’à¶šà¶±à¶±à·Š à¶´à¶§à·Šà¶§ à¶¶à¶‚', 'Mawa à·„à¶¯à¶±à·€ unga à¶‡à¶Ÿà·’à¶½à·’ kadanna à¶•à¶±à·’  à¶¸à·’à¶±à·’à·ƒà·”à¶±à·Šà¶§ mada à¶œà·„à¶±à·Šà¶± à·„à·œà¶³ à¶±à·à·„à·  ma à·€à·’à¶¯à·’à·„à¶§  ..', 'Samanaluntanam hari sathutu ethiðŸ˜‚ðŸ˜‚', 'à¶¸à·à·€ subscribe à¶šà¶»à¶± à·„à·à¶¸\\u200cà·™à¶¯à¶±à·à¶¸ à¶¸à¶¸ subscribe à¶šà¶»à¶±à·€à·\\n100% trusted , subscribe à¶šà¶»à¶½ comment 1 à¶…à¶±à·’à·€à·\\nà¶¯à·à¶œà·™à¶± à¶ºà¶±à·Šà¶± ,à¶¯à·à¶šà¶´à·” à¶œà¶¸à¶±à·Š à¶¸à¶¸ subscribe  à¶šà¶»à¶±à·€à·\\n(à¶¸à¶¸ subscribe à¶±à·œà¶šà¶»à·”à·€à·œà¶­à·Š à¶”à¶ºà·à¶½à¶§ à¶´à·”à¶½à·”à·€à¶±à·Š\\nunsubscribe  à¶šà¶»à¶±à·Šà¶± ) à¶…à¶´à·’ à¶‘à¶šà¶­à·” \\u200cà·™à·€à¶½à· subscribe 5000à¶šà·Š\\nà·„à¶¯à· à¶œà¶¸à·” bro à¶½à·...thanks for all !!!', 'à¶´à¶§à·Šà¶§à¶ºà·’ à¶šà·œà¶½à·Šà¶½à· â¤â¤', 'à¶…à¶¸à·Šà¶¸à·ðŸ˜‚', 'à¶šà·à¶¸à·šà¶šà·™ à¶…à¶» à¶¶à·œà¶»à·à·€à¶§ à·€à·”à¶©à¶¶à·”à¶»à¶± à¶šà¶±à·’à¶­à·”à¶½à·Šà¶¶à¶½à·Šà¶½ ,à¶…à¶©à·”à·€ à¶¸à·œà¶šà¶¯ à¶¢à·™à¶´à·Šà¶´à¶±à·Šà¶œà·™ à·€à·à¶©à¶±à·šà¶¯..', 'à¶…à¶´à·š à·ƒà·”à¶´à·’à¶»à·’ à¶šà·Š\\u200dà¶»à·’à¶šà¶§à·Š à¶­à¶»à·” à·ƒà·„ à¶”à·€à·”à¶±à·Šà¶œà·š à·ƒà·”à¶»à·–à¶´à·“ à¶¶à·’à¶»à·’à¶ºà¶±à·Š...          https://youtu.be/wFtO9oXQIKg', 'à¶‹à¶´à·„à·à·ƒà¶ºà¶§ à¶±à·œà·€ à¶…à¶´à·„à·à·ƒà¶ºà¶§ à¶½à¶šà·Šà¶šà·’à¶»à·’à¶¸à¶šà·Š à¶ºà¶šà·ðŸ˜€ðŸ˜€ðŸ˜€', 'à·„à·œà¶¯à¶§ à¶­à·šà¶»à·”à¶¸à·Š à¶…à¶»à¶±à·Š à¶­à·’à¶ºà¶±à·€à· à¶…à¶´à·š à¶…à¶´à·Šà¶´à¶ à·Šà¶ à·’à¶§  à¶•à¶± à¶±à·à¶§à·”à¶¸à¶šà¶§ à¶”à¶§à·Šà¶§à·”à¶ºà·’ à¶šà·’à¶ºà¶½à·à¶ºà·’ à·ƒà¶¸à¶±à¶½à¶ºà·™à¶šà·”à¶­à·Š à¶±à·™à·€à·™à¶‰  à¶­à·à¶¸à¶­à·Š à·€à·à¶© à¶´à·”à¶½à·”à·€à¶±à·Š à¶šà·’à¶ºà¶½à· à¶­à·šà¶»à·”à¶¸à·Š  à¶œà¶­à·Šà¶­à¶§ à¶œà·œà¶©à·à¶†à¶†à¶†à¶…à¶šà·Š à·ƒà·Šà¶­à·”à¶­à·’', 'pattama lassanai aaa welldone bro', 'These dogs soo good', 'Elam bro', 'Piumi & kaushi th add una nam patta machooo ðŸ‘', 'à¶œà¶­à·’à¶ºà¶šà·Š à¶­à·’à¶ºà·™à¶±à·€à· .à·„à·œà¶¯à¶ºà·’ .', 'à¶¸à·šà·€à· à·„à¶¯à¶´à·” à¶‹à¶±à·Šà¶§ à·„à·™à¶± à¶œà·„à¶±à·Šà¶± à¶•à¶±..', 'lankave\"gon\"video', 'à¶½à·ƒà·Šà·ƒà¶±à¶§ à¶šà¶»à¶½à· à¶­à·’à¶ºà·™à¶±à·€à·', 'aiyo mkda me', 'Mahinda mahathage rupe nethi unanam hodai anith ewntanam a de gelapenawamai 100/100kma', 'à¶¸à·™à¶š à¶¯à·à¶´à·”..à¶šà·à¶½à¶šà¶«à·Šà¶«à·’ à¶ºà·...à¶šà·€à·”à¶¯', 'Kowalski\\n\\n\\n\\n\\n\\n\\nkalakanni  para ballo .dapan u be ammage deewal.', 'Sri.Lankanmanikandan2020ðŸ’ºðŸ¤´ðŸ¦ðŸ¦ðŸ¦ðŸ¦ðŸ¦ðŸ¦ðŸ¦ðŸ¦ðŸ¦ðŸ¦ðŸ¦ðŸ¦ðŸ¦ðŸ¦ðŸ¦â‡â‡â‡â‡âœ´âœ´âœ´âœ´âœ´âœ´âœ´âœ´âœ´âœ´âœ´âœ´âœ¡âœ¡ðŸ•‰ðŸ¤´ðŸ’º', 'all are sri lanka`s clowns', 'à¶”à¶šà·Šà¶šà·œà¶¸ à¶…à·€à·”à¶½à¶šà·Š ne ban eth à¶¸à·„à·’à¶±à·Šà¶¯ à¶¸à·„à¶­à·Šà¶­à¶ºà· à¶œà¶­à·Šà¶­ à¶‘à¶š set wen ne ban à¶»à¶§ bereagaththa miniha mona à·€à·à¶»à·à¶¯à·’ à¶­à·’à¶¶à·”à¶«à¶­à·Š', 'aka patta bro pattetaðŸ˜˜ðŸ˜˜ðŸ˜˜ðŸ˜˜', 'aiyooo ane ai me baninne? fun ekatane danne.... gd job guy... funny one', 'ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚  à¶…à¶ºà·’à¶ºà·   à¶½à·ƒà·Šà·ƒà¶±à¶ºà·’', 'à¶…à¶©à·”à¶šà·”à¶½à·š à¶´à¶»à·„à·à¶­à·Šà¶­ à¶­à¶¸à¶ºà·’ à¶¸à·š à·€à·’à¶¯à·’à¶ºà¶§ à¶…à¶´à·„à·ƒ à¶šà¶»à¶±à·Šà¶±à·š à¶»à·à¶©à·’à¶ºà· .à¶…à·„à·’à¶šà·”à¶«à·Šà¶¨à·’à¶šà¶ºà¶±à·Š', 'Supiri ðŸ˜‚ðŸ˜‚ðŸ˜‚', 'Ammooo eka', 'Niyamai... ape rata gena katha krl vadak ne..ðŸ‘ðŸ‘ðŸ‘', 'ðŸ˜‚ðŸ˜‚ðŸ˜‚ à¶¯à·™à¶±à·Šà¶±à¶§à¶¸ à¶¸à¶© à¶œà·„à¶½à·', 'à¶‘à¶½à·Š à¶‘à¶½à·Š à¶•à¶ºà·’.à¶œà·à¶¸à·Šà¶¸', 'ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚', 'mara lassanai', 'Dancers all one poudumayðŸ˜‚ðŸ˜ˆðŸ˜¨ðŸ˜¤', 'MAWAH THAMAI LANKAWEH RAJEA DANCE 2018 ALA ALA', 'Mula amataka krapu musala hatta', 'kauri monawa kiuwath me rara mahinda rajura bara wewa', 'à¶”à¶¶à¶§ à¶¢à¶ºà·™à¶±à·Š à¶¢à¶º à¶¶à·”à¶¯à·” à·ƒà¶»à¶«à¶ºà·’. https://goo.gl/Zfj1hZ', 'à¶šà·€à·”à¶»à·” à¶‹à¶±à¶­à·Š à¶¸à·’à¶±à·’à·ƒà·”à¶±à·Š à¶’ à¶…à¶º à¶šà¶»à¶´à·” à¶¯à·šà·€à¶½à·Š à¶’ à¶…à¶ºà¶§ à¶­à·’à¶ºà·™à¶ºà·’ à¶…à¶±à·’à¶š à¶¸à·š à¶…à¶º à¶…à¶­à¶» à¶»à¶§à¶§ à·ƒà·šà·€à¶ºà¶šà·Š à¶šà¶»à¶´à·” à¶…à¶ºà¶­à·Š à¶‰à¶±à·Šà¶±à·€à· à¶»à¶§ à¶†à¶´à¶ºà¶šà·’à¶±à·Š à¶¶à·šà¶»à¶œà¶­à·Šà¶­ à¶…à¶ºà¶­à·Š à¶‰à¶±à·Šà¶±à·€à· à·€à·à¶»à¶¯à·’ à¶¯à·à·„à¶šà·Š à¶…à·ƒà·Šà·ƒà·™ à¶‘à¶š à·„à·œà¶¯à¶šà·Š à·„à¶»à·’ à¶¯à¶šà·’à¶±à·Šà¶± à¶´à·”à¶»à·”à¶¯à·” à·€à·™à¶±à·Šà¶± à¶¸à·’à¶±à·’à·ƒà·”à¶±à·Šà¶§ à¶…à¶´à·„à·à·ƒ à¶šà¶»à¶½à· à¶¸à¶© à¶œà·„à¶½à· à¶…à¶šà·”à·ƒà¶½à·Š à¶´à·”à¶»à·€à¶œà¶±à·Šà¶± à¶‘à¶´à· à¶’à¶š à¶¸à·„ à¶´à·€à¶šà·Š', 'Meka num naraka vadak', 'Thendi patty puri mon', 'Karapu eka ponnayek', 'Shok ane oyala...', 'me wage dewal dana un hire daanna uni..', 'à¶´à¶§à·Šà¶§ à¶†à¶­à¶½à·Š à¶‘à¶šà¶šà·Š 0777192697', 'www.bruise lee', 'kalakanni para ballo dapan ube ammage deewal ..pakya.', 'Top', 'If \\n X', 'daru senr', 'balako ape janapati mhinda koma haduwath handi...ðŸ˜ðŸ˜ðŸ˜', 'ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚', 'à¶´à·€à·’à¶šà·à¶»à¶ºà·’à¶±à·Šà¶œà·š  à¶´à·€à·’  à·€à·à¶©', 'Hahahahaha', 'à¶ à·à¶§à¶»à·Š à¶¶à¶±à·Š', 'à¶±à¶»à¶š. à·€à·à¶©à¶šà·Š', 'Athal bn meka.....', 'ðŸ˜¢ðŸ˜¢ðŸ˜¢ðŸ˜¢ðŸ˜¢', 'supri ea', 'Ammoo aka', 'meka hadapu video edit soffweya eke nama mokakda bn', 'à¶±à·’à¶ºà¶¸à¶ºà·’.à·„à¶½à·.à¶‘à¶ à·’à¶ à¶».à¶ à·à¶§à¶»à·Š.à¶±à·.', 'Super ðŸ˜¥ðŸ˜¥ðŸ˜¥ðŸ˜¥', 'mun honda naha tamar aath oya widiyata waradi kiyana eka tamage pahathkama pradarsanaya kireemak. ....', 'ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚', \"Mahenda ayi Anna dalda malegwa Ra's padennda modya\", 'hitagena\"\"ati\"loku\"         vadak\"\"kare\"kiya\"metana\"inna\"eka\"amati\"\"kenek\"karapu\"deyak\"oyalata\"karanna\"puluvanda\"vikara\"ova\"\"\"\"\"\"\"da\"gnnawa\"\"ara\"sunil\"pererata', 'Kohomath mahinda weddek thamai', 'ponna kariyo ammata hukanna kiyahan hadapu ekage ammata polimak danna ennam', 'Ane melody eka godak lassanai.me melody eka thiyena song ekak thiyenawada', 'Ammo eka', '\\u2063à¶­à¶¯  à¶­à¶¯', 'à¶½à¶¶à¶šà¶­à·Š', 'à¶‘à¶½ à¶‘à¶½', 'very nice song, à¶šà·œà·„à·œà¶¸à¶¯ à¶¸à·šà¶š https://youtu.be/aU7I0v0muns', 'danne akak thibbama nadde yakoo', 'Rani kuwat niyamai wauuuuuuuuuuuðŸ˜™ðŸ˜™ðŸ˜™ðŸ˜™ðŸ˜™ðŸ˜™ðŸ˜™ðŸ˜™ðŸ˜™ðŸ˜†ðŸ˜—ðŸ˜†ðŸ˜†ðŸ˜†ðŸ˜†ðŸŒ¹ðŸŒ¹ðŸŒ¹ðŸŒ¹ðŸŒ¹ðŸŒ¹ðŸŒ¹ðŸŒ¹ðŸŒ¹ðŸŒ¹ðŸ‘ðŸ‘ðŸ‘ðŸ‘ðŸ‘ðŸ‘ðŸ‘ðŸ‘ðŸ‘ðŸ‘ðŸ‘ðŸ‘ðŸ’ªðŸ’ªðŸ’ªðŸ’ªðŸ’ªðŸ’ª', 'Hahaha', 'Siresanaapai  pradana dancer', 'mara hina brooo', 'tho pilawa yuddayan bara gattha charithawalata mahemadasalakanne', 'He hee ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚', 'ðŸ‘ŒðŸ‘ŒðŸ‘ŒðŸ‘ŒðŸ‘ŒðŸ‘ŒðŸ‘Œ', 'horahatekre-ratakapuyakku', 'Nidahase jewath wenna rata berala  deviyek wage minisekuta mewage apahasa karana unge amma appath mewagem apahasayata lakwenawata kemathi athi buru weda karanna epa manussayek wela', 'Vimal viravanshaya mn ekata gala penava', 'Apith hoyanne sungaraya awul karapu ekawai', 'JVP   thamay meyalage katu kala abarila enne', 'mula amathaka wela wage thopita neeee', 'à·ƒà·”à¶´à·’à¶»à·’.........', 'Ane mandanne ne', 'ana mandaðŸ˜‘ðŸ˜‘ðŸ˜‘ðŸ˜‘ðŸ˜‘ðŸ˜‘', 'Kawda meka haduwa ponaya.minissunta mada gahanna apa..ponnaya..'], \n",
    "    '_id': '5d9ad961112acd12bffdf83d'\n",
    "}\n",
    "\n",
    "comments = bihara_dict.get('Comment')\n",
    "print(len(comments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "84\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "pos_count = 0\n",
    "neg_count = 0\n",
    "for i in comments:\n",
    "    Test_comment_1 = i\n",
    "    test_com_1 = []\n",
    "    for r in Test_comment_1.split('\\u0020'):\n",
    "        test_com_1.append(r)\n",
    "    test_dict_1 = dict([(word,True) for word in test_com_1 ])\n",
    "    last_classifier = classifer_detail(classifier_train_uni.classify(test_dict_1),\n",
    "                                   classifier_train_bi.classify(test_dict_1),\n",
    "                                   LogisticRegression_classifier_uni.classify(test_dict_1),\n",
    "                                   LogisticRegression_classifier_bi.classify(test_dict_1),\n",
    "                                   LinearSVC_classifier_uni.classify(test_dict_1),\n",
    "                                   LinearSVC_classifier_bi.classify(test_dict_1)\n",
    "                                  )\n",
    "#     print(last_classifier)\n",
    "    if (last_classifier) == 'negative':\n",
    "        neg_count+=1\n",
    "    elif (last_classifier) == 'positive':\n",
    "        pos_count+=1\n",
    "        \n",
    "print (pos_count) \n",
    "print (neg_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test_comment_1 = comments[2]\n",
    "# print (Test_comment_1)\n",
    "# test_com_1 = []\n",
    "# for r in Test_comment_1.split('\\u0020'):\n",
    "#     test_com_1.append(r)\n",
    "# test_dict_1 = dict([(word,True) for word in test_com_1 ])\n",
    "# last_classifier = classifer_detail(classifier_train_uni.classify(test_dict_1),\n",
    "#                                    classifier_train_bi.classify(test_dict_1),\n",
    "#                                    LogisticRegression_classifier_uni.classify(test_dict_1),\n",
    "#                                    LogisticRegression_classifier_bi.classify(test_dict_1),\n",
    "#                                    LinearSVC_classifier_uni.classify(test_dict_1),\n",
    "#                                    LinearSVC_classifier_bi.classify(test_dict_1)\n",
    "#                                   )\n",
    "\n",
    "# print(last_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classifer_detail(NBclassifer_Uni,NBclassifer_Bi,LR_Uni,LR_Bi,SVM_Uni,SVM_Bi):\n",
    "    \n",
    "    positive_votes = 0\n",
    "    negative_votes = 0\n",
    "    \n",
    "    \n",
    "    if NBclassifer_Uni == 'positive':\n",
    "        positive_votes +=1\n",
    "    else:\n",
    "        negative_votes +=1\n",
    "        \n",
    "    if NBclassifer_Bi == 'positive':\n",
    "        positive_votes +=1\n",
    "    else:\n",
    "        negative_votes +=1\n",
    "\n",
    "        \n",
    "    if LR_Uni == 'positive':\n",
    "        positive_votes +=1\n",
    "    else:\n",
    "        negative_votes +=1\n",
    "        \n",
    "    if LR_Bi == 'positive':\n",
    "        positive_votes +=1\n",
    "    else:\n",
    "        negative_votes +=1    \n",
    "        \n",
    "        \n",
    "    if SVM_Uni == 'positive':\n",
    "        positive_votes +=1\n",
    "    else:\n",
    "        negative_votes +=1\n",
    "        \n",
    "    if SVM_Bi == 'positive':\n",
    "        positive_votes +=1\n",
    "    else:\n",
    "        negative_votes +=1    \n",
    "        \n",
    "    if positive_votes>negative_votes:\n",
    "        return 'positive'\n",
    "    elif positive_votes<negative_votes:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'positive'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
