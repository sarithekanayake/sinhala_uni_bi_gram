{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "import random\n",
    "import codecs\n",
    "import re\n",
    "import glob\n",
    "import string\n",
    "import io\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC,NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "File handeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comments = open(\"F:\\\\DOCUMENTS\\\\Python\\\\myfiles\\\\NLTK\\\\positive.txt\",encoding='utf-8').read()\n",
    "neg_comments = open(\"F:\\\\DOCUMENTS\\\\Python\\\\myfiles\\\\NLTK\\\\negative.txt\",encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Line append into arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_comments_list = list(pos_comments.split('\\n'))\n",
    "neg_comments_list = list(neg_comments.split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Feature Extraction for Uni-Gram and Bi-Gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_features_unigram(words):\n",
    "    unigram_dict = dict((word,True) for word in words.split('\\u0020'))     \n",
    "    return unigram_dict\n",
    "\n",
    "\n",
    "def create_word_features_bi_gram(words):\n",
    "    raw = []\n",
    "    bi_gram = []\n",
    "    i=0\n",
    "    \n",
    "    for w in words.split('\\u0020'):\n",
    "        raw.append(w)\n",
    "\n",
    "    while i<(len(raw)-1):\n",
    "        bi_gram.append(raw[i] + \" \"+raw[i+1])\n",
    "        i+=1\n",
    "     \n",
    "    bigram_dict = dict((word,True) for word in bi_gram)\n",
    "    return bigram_dict\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Positive and Negative features array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word_features_uni = []\n",
    "neg_word_features_uni = []    \n",
    "pos_word_features_bi = []\n",
    "neg_word_features_bi = []    \n",
    "\n",
    "\n",
    "for i in pos_comments_list:\n",
    "    pos_word_features_uni.append((create_word_features_unigram(i),\"positive\"))\n",
    "    pos_word_features_bi.append((create_word_features_bi_gram(i),\"positive\"))\n",
    "    \n",
    "for i in neg_comments_list:\n",
    "    neg_word_features_uni.append((create_word_features_unigram(i),\"negative\"))\n",
    "    neg_word_features_bi.append((create_word_features_bi_gram(i),\"negative\"))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Setup trainning and testing test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_features_uni = pos_word_features_uni + neg_word_features_uni\n",
    "random.shuffle(all_word_features_uni)\n",
    "trainning_set_uni = all_word_features_uni[:200]\n",
    "test_set_uni = all_word_features_uni[200:290]\n",
    "\n",
    "all_word_features_bi = pos_word_features_bi + neg_word_features_bi\n",
    "random.shuffle(all_word_features_bi)\n",
    "trainning_set_bi = all_word_features_bi[:200]\n",
    "test_set_bi = all_word_features_bi[200:290]\n",
    "\n",
    "# print(\" \",len(all_word_features_uni))\n",
    "# print(\" \",len(all_word_features_bi))\n",
    "# print(\" \",len(pos_word_features_uni),len(neg_word_features_uni))\n",
    "# print(\" \",len(pos_word_features_bi) ,len(neg_word_features_bi))\n",
    "# print(pos_word_features_bi[155])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Naive Bayes Accuracy check for Uni-Gram, Bi-Gram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes classifier UniGram:  71.11111111111111\n",
      "Most Informative Features\n",
      "                  ආදරෙයි = True           positi : negati =      7.0 : 1.0\n",
      "                     රටේ = True           negati : positi =      6.0 : 1.0\n",
      "                  ලංකාවේ = True           negati : positi =      6.0 : 1.0\n",
      "                    අපිට = True           negati : positi =      5.2 : 1.0\n",
      "                     අනේ = True           negati : positi =      4.4 : 1.0\n",
      "\n",
      "NaiveBayes classifier Bi-Gram :  61.111111111111114\n",
      "Most Informative Features\n",
      "            ක්‍රිකට් වලට = True           negati : positi =      2.6 : 1.0\n",
      "               ගොන් ආතල් = True           negati : positi =      2.6 : 1.0\n",
      "             ක්‍රීඩා කරන = True           negati : positi =      1.9 : 1.0\n",
      "                  මේ වගේ = True           negati : positi =      1.9 : 1.0\n",
      "          කුසල් මෙන්ඩිස් = True           negati : positi =      1.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_train_uni = NaiveBayesClassifier.train(trainning_set_uni)\n",
    "accuracy_nb_uni = nltk.classify.util.accuracy(classifier_train_uni,test_set_uni)\n",
    "print(\"NaiveBayes classifier UniGram: \", (accuracy_nb_uni * 100))\n",
    "classifier_train_uni.show_most_informative_features(5)\n",
    "\n",
    "classifier_train_bi = NaiveBayesClassifier.train(trainning_set_bi)\n",
    "accuracy_nb_bi = nltk.classify.util.accuracy(classifier_train_bi,test_set_bi)\n",
    "print(\"\\nNaiveBayes classifier Bi-Gram : \", (accuracy_nb_bi * 100))\n",
    "classifier_train_bi.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Logisitc Regression Classifier Accuracy check for Uni-Gram, Bi-Gram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression classifier Uni Gram:  75.55555555555556\n",
      "Logistic Regression classifier Bi-Gram:  57.77777777777777\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_classifier_uni = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier_uni.train(trainning_set_uni)\n",
    "accuracy_logistic_uni = nltk.classify.accuracy(LogisticRegression_classifier_uni,test_set_uni)\n",
    "print(\"Logistic Regression classifier Uni Gram: \",accuracy_logistic_uni*100)\n",
    "\n",
    "LogisticRegression_classifier_bi = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier_bi.train(trainning_set_bi)\n",
    "accuracy_logistic_bi = nltk.classify.accuracy(LogisticRegression_classifier_bi,test_set_bi)\n",
    "print(\"Logistic Regression classifier Bi-Gram: \",accuracy_logistic_bi*100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "SVC classifier Accuracy check for Uni-Gram, Bi-Gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM classifier Uni Gram:  71.11111111111111\n",
      "SVM classifier Algo Bi-Gram:  57.77777777777777\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier_uni = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier_uni.train(trainning_set_uni)\n",
    "accuracy_svm_uni = nltk.classify.accuracy(LinearSVC_classifier_uni,test_set_uni)\n",
    "print(\"SVM classifier Uni Gram: \",accuracy_svm_uni*100)\n",
    "\n",
    "LinearSVC_classifier_bi = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier_bi.train(trainning_set_bi)\n",
    "accuracy_svm_bi = nltk.classify.accuracy(LinearSVC_classifier_bi,test_set_bi)\n",
    "print(\"SVM classifier Algo Bi-Gram: \",accuracy_svm_bi*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Testing for Positive,Negative comments in Unigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "අපි ඔබ සමඟ සිටිමු (Unigram): positive\n",
      "අපිට දැන් අප්පිරියයි (Unigram): negative\n"
     ]
    }
   ],
   "source": [
    "Test_comment_1 = '''අපි ඔබ සමඟ සිටිමු'''\n",
    "test_com_1 = []\n",
    "for r in Test_comment_1.split('\\u0020'):\n",
    "    test_com_1.append(r)\n",
    "test_dict_1 = dict([(word,True) for word in test_com_1 ])\n",
    "print(Test_comment_1 +\" (Unigram): \"+ classifier_train_uni.classify(test_dict_1))\n",
    "\n",
    "\n",
    "test_comment_2 = '''අපිට දැන් අප්පිරියයි'''\n",
    "testing_2 = []\n",
    "for r in test_comment_2.split('\\u0020'):\n",
    "    testing_2.append(r)\n",
    "test_dict_2 = dict([(word,True) for word in testing_2 ])\n",
    "print(test_comment_2 +\" (Unigram): \"+ classifier_train_uni.classify(test_dict_2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Testing for Positive,Negative comments in Bi-Gram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "අපි ඔබ සමඟ සිටිමු (Bi-Gram) : positive\n",
      "අපිට දැන් අප්පිරියයි (Bi-Gram) : positive\n"
     ]
    }
   ],
   "source": [
    "Test_comment_1 = '''අපි ඔබ සමඟ සිටිමු'''\n",
    "count = 0\n",
    "test_com_1 = []\n",
    "for r in Test_comment_1.split('\\u0020'):\n",
    "    test_com_1.append(r)\n",
    "bi_grammed_1 = []\n",
    "\n",
    "while count<(len(test_com_1)-1):\n",
    "        bi_grammed_1.append(test_com_1[count] + \" \"+test_com_1[count+1])\n",
    "        count+=1\n",
    "test_dict_1 = dict([(word,True) for word in bi_grammed_1 ])\n",
    "print (Test_comment_1 +\" (Bi-Gram) : \"+classifier_train_bi.classify(test_dict_1))\n",
    "\n",
    "\n",
    "test_comment_2 = '''අපිට දැන් අප්පිරියයි'''\n",
    "count = 0\n",
    "test_com_2 = []\n",
    "for r in test_comment_2.split('\\u0020'):\n",
    "    test_com_2.append(r)    \n",
    "bi_grammed_2 = []\n",
    "while count<(len(test_com_2)-1):\n",
    "        bi_grammed_2.append(test_com_2[count] + \" \"+test_com_2[count+1])\n",
    "        count+=1\n",
    "test_dict_2 = dict([(word,True) for word in bi_grammed_2 ])\n",
    "print (test_comment_2 +\" (Bi-Gram) : \"+classifier_train_bi.classify(test_dict_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "bihara_dict = {\n",
    "    'Title': 'Shrungaaraya   Mahinda My3 Ranil ;-)',\n",
    "    'View_Count': '2738691', \n",
    "    'Likes': '5350',\n",
    "    'DisLikes': '1733', \n",
    "    'VideoID': 'uJRG1512ygc', \n",
    "    'Comment': ['අපි ඔබ සමඟ සිටිමු','අපිට දැන් අප්පිරියයි'],\n",
    "    '_id': '5d9ad961112acd12bffdf83d'\n",
    "}\n",
    "\n",
    "comments = bihara_dict.get('Comment')\n",
    "print(len(comments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "අපි ඔබ සමඟ සිටිමු positive\n",
      "අපිට දැන් අප්පිරියයි negative\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "pos_comment_count = 0\n",
    "neg_comment_count = 0\n",
    "for i in comments:\n",
    "    comment = i\n",
    "    splitted_comments = []\n",
    "    for r in comment.split('\\u0020'):\n",
    "        splitted_comments.append(r)\n",
    "    comments_dictionary = dict([(word,True) for word in splitted_comments ])\n",
    "    vote_classifier_output = Voting_classifer(classifier_train_uni.classify(comments_dictionary),\n",
    "                                   classifier_train_bi.classify(comments_dictionary),\n",
    "                                   LogisticRegression_classifier_uni.classify(comments_dictionary),\n",
    "                                   LogisticRegression_classifier_bi.classify(comments_dictionary),\n",
    "                                   LinearSVC_classifier_uni.classify(comments_dictionary),\n",
    "                                   LinearSVC_classifier_bi.classify(comments_dictionary),\n",
    "                                   accuracy_nb_uni,accuracy_nb_bi,\n",
    "                                   accuracy_logistic_uni, accuracy_logistic_bi,\n",
    "                                   accuracy_svm_uni, accuracy_svm_bi\n",
    "                                  )\n",
    "\n",
    "    \n",
    "    if (vote_classifier_output) == 'negative':\n",
    "        neg_comment_count+=1\n",
    "    elif (vote_classifier_output) == 'positive':\n",
    "        pos_comment_count+=1\n",
    "        \n",
    "print (pos_comment_count) \n",
    "print (neg_comment_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stop_word_remover(comment):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Voting_classifer(NBclassifer_Uni,NBclassifer_Bi,LR_Uni,LR_Bi,SVM_Uni,SVM_Bi,acc_nb_uni,acc_nb_bi,acc_lr_uni,acc_lr_bi,\n",
    "                    acc_svm_uni,acc_svm_bi):\n",
    "    \n",
    "    positive_votes = 0\n",
    "    negative_votes = 0\n",
    "    \n",
    "    classifier_mode = [NBclassifer_Uni,NBclassifer_Bi,LR_Uni,LR_Bi,SVM_Uni,SVM_Bi]\n",
    "    \n",
    "    if NBclassifer_Uni == 'positive': positive_votes +=acc_nb_uni \n",
    "    else: negative_votes +=acc_nb_uni      \n",
    "    if NBclassifer_Bi == 'positive':positive_votes +=acc_nb_bi\n",
    "    else:negative_votes +=acc_nb_bi      \n",
    "    if LR_Uni == 'positive': positive_votes +=acc_lr_uni\n",
    "    else:  negative_votes +=acc_lr_uni       \n",
    "    if LR_Bi == 'positive':  positive_votes +=acc_lr_bi\n",
    "    else:  negative_votes +=acc_lr_bi         \n",
    "    if SVM_Uni == 'positive':positive_votes +=acc_svm_uni\n",
    "    else: negative_votes +=acc_svm_uni    \n",
    "    if SVM_Bi == 'positive': positive_votes +=acc_svm_bi\n",
    "    else: negative_votes +=acc_svm_bi     \n",
    "    if positive_votes>negative_votes: return ('positive')\n",
    "    elif positive_votes<negative_votes:return ('negative')\n",
    "    else:  return (mode(classifier_mode))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
